{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- clean text\n",
    "- tf-idf text for weights\n",
    "- embeddings\n",
    "    - word2vec words * tf-idf\n",
    "    - save to each row of each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') # for part of speech tagging, required for lemmatization\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "# import gensim.sklearn_api\n",
    "import gensim.models.phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/cleaned/'\n",
    "DF_NAME = 'concatenated_df.pkl'\n",
    "\n",
    "with open(DATA_PATH + DF_NAME, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user</th>\n",
       "      <th>id_str</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>Hash words</th>\n",
       "      <th>link</th>\n",
       "      <th>entities</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>urls</th>\n",
       "      <th>no_link</th>\n",
       "      <th>...</th>\n",
       "      <th>domains</th>\n",
       "      <th>day_of_tweet</th>\n",
       "      <th>month_year</th>\n",
       "      <th>week_month_year</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link_shorteners</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31 23:08:33+00:00</td>\n",
       "      <td>{'id': 19031057, 'id_str': '19031057', 'name':...</td>\n",
       "      <td>1223382589689356288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#coronavirus</td>\n",
       "      <td>https://twitter.com/user/status/12233825896893...</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>(43, 0.999852366207409)</td>\n",
       "      <td>[https://www.kron4.com/news/national/when-will...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[www.kron4.com]</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>19031057</td>\n",
       "      <td>43</td>\n",
       "      <td>False</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>KRON4 by: Aubree Gordon, University of Michig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-31 23:11:29+00:00</td>\n",
       "      <td>{'id': 798925214, 'id_str': '798925214', 'name...</td>\n",
       "      <td>1223383328843280384</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 20:41:20 +0000 2020...</td>\n",
       "      <td>#coronavirus #vaccine:</td>\n",
       "      <td>https://twitter.com/user/status/12233833288432...</td>\n",
       "      <td>{'hashtags': [{'text': 'Coronavirus', 'indices...</td>\n",
       "      <td>(6, 0.9883827140131066)</td>\n",
       "      <td>[https://www.cnbc.com/2020/01/31/coronavirus-w...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[www.cnbc.com]</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>798925214</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>[Coronavirus, vaccine]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>U.S. and international health officials are s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-31 23:12:03+00:00</td>\n",
       "      <td>{'id': 806144538049970176, 'id_str': '80614453...</td>\n",
       "      <td>1223383471999127552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233834719991...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(74, 0.9999928079220538)</td>\n",
       "      <td>[https://www.businessinsider.com/australia-suc...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[www.businessinsider.com]</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>806144538049970176</td>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>A leading-edge research firm focused on digit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-31 23:35:42+00:00</td>\n",
       "      <td>{'id': 61298849, 'id_str': '61298849', 'name':...</td>\n",
       "      <td>1223389423045206016</td>\n",
       "      <td>{'created_at': 'Tue Jan 28 18:26:16 +0000 2020...</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233894230452...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(36, 0.9999993854913211)</td>\n",
       "      <td>[https://www.greenmedinfo.com/blog/examining-r...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[www.greenmedinfo.com]</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>61298849</td>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>Mr. Kennedy is in very safe territory by rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-31 23:37:23+00:00</td>\n",
       "      <td>{'id': 1152822375567654912, 'id_str': '1152822...</td>\n",
       "      <td>1223389844719599616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233898447195...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(74, 0.9999985797901156)</td>\n",
       "      <td>[https://www.dailymail.co.uk/news/article-7952...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[www.dailymail.co.uk]</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1152822375567654912</td>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>By Kylie Stevens and Stephen Gibbs and Nic Wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "0  2020-01-31 23:08:33+00:00   \n",
       "1  2020-01-31 23:11:29+00:00   \n",
       "2  2020-01-31 23:12:03+00:00   \n",
       "3  2020-01-31 23:35:42+00:00   \n",
       "4  2020-01-31 23:37:23+00:00   \n",
       "\n",
       "                                                user               id_str  \\\n",
       "0  {'id': 19031057, 'id_str': '19031057', 'name':...  1223382589689356288   \n",
       "1  {'id': 798925214, 'id_str': '798925214', 'name...  1223383328843280384   \n",
       "2  {'id': 806144538049970176, 'id_str': '80614453...  1223383471999127552   \n",
       "3  {'id': 61298849, 'id_str': '61298849', 'name':...  1223389423045206016   \n",
       "4  {'id': 1152822375567654912, 'id_str': '1152822...  1223389844719599616   \n",
       "\n",
       "                                    retweeted_status              Hash words  \\\n",
       "0                                                NaN            #coronavirus   \n",
       "1  {'created_at': 'Fri Jan 31 20:41:20 +0000 2020...  #coronavirus #vaccine:   \n",
       "2                                                NaN             No hashtags   \n",
       "3  {'created_at': 'Tue Jan 28 18:26:16 +0000 2020...             No hashtags   \n",
       "4                                                NaN             No hashtags   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://twitter.com/user/status/12233825896893...   \n",
       "1  https://twitter.com/user/status/12233833288432...   \n",
       "2  https://twitter.com/user/status/12233834719991...   \n",
       "3  https://twitter.com/user/status/12233894230452...   \n",
       "4  https://twitter.com/user/status/12233898447195...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'hashtags': [{'text': 'coronavirus', 'indices...   \n",
       "1  {'hashtags': [{'text': 'Coronavirus', 'indices...   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                Topic Label  \\\n",
       "0   (43, 0.999852366207409)   \n",
       "1   (6, 0.9883827140131066)   \n",
       "2  (74, 0.9999928079220538)   \n",
       "3  (36, 0.9999993854913211)   \n",
       "4  (74, 0.9999985797901156)   \n",
       "\n",
       "                                                urls  no_link  ...  \\\n",
       "0  [https://www.kron4.com/news/national/when-will...    False  ...   \n",
       "1  [https://www.cnbc.com/2020/01/31/coronavirus-w...    False  ...   \n",
       "2  [https://www.businessinsider.com/australia-suc...    False  ...   \n",
       "3  [https://www.greenmedinfo.com/blog/examining-r...    False  ...   \n",
       "4  [https://www.dailymail.co.uk/news/article-7952...    False  ...   \n",
       "\n",
       "                     domains day_of_tweet month_year week_month_year  \\\n",
       "0            [www.kron4.com]   2020-01-31    2020-01               5   \n",
       "1             [www.cnbc.com]   2020-01-31    2020-01               5   \n",
       "2  [www.businessinsider.com]   2020-01-31    2020-01               5   \n",
       "3     [www.greenmedinfo.com]   2020-01-31    2020-01               5   \n",
       "4      [www.dailymail.co.uk]   2020-01-31    2020-01               5   \n",
       "\n",
       "               user_id  cluster is_retweet                hashtags  \\\n",
       "0             19031057       43      False           [coronavirus]   \n",
       "1            798925214        6       True  [Coronavirus, vaccine]   \n",
       "2   806144538049970176       74      False                  [None]   \n",
       "3             61298849       36       True                  [None]   \n",
       "4  1152822375567654912       74      False                  [None]   \n",
       "\n",
       "   link_shorteners                                       article_text  \n",
       "0          [False]   KRON4 by: Aubree Gordon, University of Michig...  \n",
       "1          [False]   U.S. and international health officials are s...  \n",
       "2          [False]   A leading-edge research firm focused on digit...  \n",
       "3          [False]    Mr. Kennedy is in very safe territory by rep...  \n",
       "4           [True]   By Kylie Stevens and Stephen Gibbs and Nic Wh...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent_to_words(sentences):\n",
    "#     \"\"\"Gensim simple preprocess. Turns each element in list of strings to a list of tokens. removes punct\"\"\"\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "def simple_tokenize(texts):\n",
    "    doc_toks = []\n",
    "    for i in texts:\n",
    "        doc_toks.append(list(gensim.utils.tokenize(i, deacc = True, lowercase=True))) # where i is a string\n",
    "    return doc_toks\n",
    "\n",
    "\n",
    "def remove_stopwords_and_simple_preprocess(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "#     common = []\n",
    "#     with open('./data/raw/common_words.csv', 'r') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         for line in reader:\n",
    "#             common.extend(line)\n",
    "#     stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'employ', 'benefit'])\n",
    "#     stop_words.extend(common)\n",
    "    return [[word for word in simple_preprocess(str(doc), deacc = True) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLTK Lemmatize with POS Tag\n",
    "# def get_wordnet_pos(word):\n",
    "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "#     tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "#     tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                 \"N\": wordnet.NOUN,\n",
    "#                 \"V\": wordnet.VERB,\n",
    "#                 \"R\": wordnet.ADV}\n",
    "\n",
    "#     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# def lemmatization(texts): # texts is list of lists\n",
    "#     t0 = time.time()\n",
    "#     print('lemmatizing...')\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     texts_out = []\n",
    "#     for doc in texts: # the document is a list\n",
    "#         texts_out.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc])\n",
    "#     print('lemmatization complete')\n",
    "#     print('{} seconds'.format(time.time()-t0))\n",
    "#     return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_clean(data):\n",
    "    \"\"\"data is a list of strings, uncleaned, each element a document\"\"\"\n",
    "\n",
    "    # tokenize, lowercase, remove punct, keep common words for bigram building\n",
    "    t0 = time.time()\n",
    "    print('simple tokenizing...')\n",
    "    data_words = simple_tokenize(data)\n",
    "    print('simple_tokenizing took {} seconds'.format(time.time()-t0))\n",
    "    \n",
    "   \n",
    "    ### Build bigram model\n",
    "    t0 = time.time()\n",
    "    print('building bigrams...')\n",
    "    common_connectors = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "    bigram = gensim.models.Phrases(data_words, \n",
    "                                   min_count=25, \n",
    "                                   threshold=100, \n",
    "#                                    connector_words=common_connectors) # let common eng articles connect words\n",
    "                                   common_terms = common_connectors)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    data_words_bigrams = [bigram_mod[doc] for doc in data_words]\n",
    "    print('building bigrams took {} seconds'.format(time.time()-t0))\n",
    "\n",
    "    ### Build trigram model\n",
    "    t0 = time.time()\n",
    "    print('building trigrams...')\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], \n",
    "                                    min_count=25, \n",
    "                                    threshold=100, \n",
    "#                                     connector_words = common_connectors)\n",
    "                                    common_terms = common_connectors)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    data_words_trigrams = [trigram_mod[bigram_mod[doc]] for doc in data_words_bigrams]\n",
    "    print('building trigrams took {} seconds'.format(time.time()-t0))\n",
    "    \n",
    "    \n",
    "    # remake into strings\n",
    "    texts = []\n",
    "    for i in data_words_trigrams:\n",
    "        string = ' '.join(i)\n",
    "        texts.append(string)\n",
    "    \n",
    "    \n",
    "    # Remove Stop Words\n",
    "    t0 = time.time()\n",
    "    print('remove_stopwords_and_simple_preprocess...')\n",
    "    data_words_trigrams_nostops = remove_stopwords_and_simple_preprocess(texts)\n",
    "    print('remove_stopwords_and_simple_preprocess took {} seconds'.format(time.time()-t0))\n",
    "\n",
    "    # return a lemmatized list of unigram + bigram + trigrams\n",
    "#     return lemmatization(data_words_trigrams)\n",
    "    return data_words_trigrams_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple tokenizing...\n",
      "simple_tokenizing took 50.34017825126648 seconds\n",
      "building bigrams...\n",
      "building bigrams took 88.33401918411255 seconds\n",
      "building trigrams...\n",
      "building trigrams took 192.49179601669312 seconds\n",
      "remove_stopwords_and_simple_preprocess...\n",
      "remove_stopwords_and_simple_preprocess took 70.06075119972229 seconds\n"
     ]
    }
   ],
   "source": [
    "df['article_text_cleaned'] = full_clean(df['article_text'].tolist())\n",
    "\n",
    "# simple_tokenizing took 50.34017825126648 seconds\n",
    "# building bigrams took 88.33401918411255 seconds\n",
    "# building trigrams took 192.49179601669312 seconds\n",
    "# remove_stopwords_and_simple_preprocess took 70.06075119972229 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user</th>\n",
       "      <th>id_str</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>Hash words</th>\n",
       "      <th>link</th>\n",
       "      <th>entities</th>\n",
       "      <th>Topic Label</th>\n",
       "      <th>urls</th>\n",
       "      <th>no_link</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_tweet</th>\n",
       "      <th>month_year</th>\n",
       "      <th>week_month_year</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link_shorteners</th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-31 23:08:33+00:00</td>\n",
       "      <td>{'id': 19031057, 'id_str': '19031057', 'name':...</td>\n",
       "      <td>1223382589689356288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#coronavirus</td>\n",
       "      <td>https://twitter.com/user/status/12233825896893...</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>(43, 0.999852366207409)</td>\n",
       "      <td>[https://www.kron4.com/news/national/when-will...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>19031057</td>\n",
       "      <td>43</td>\n",
       "      <td>False</td>\n",
       "      <td>[coronavirus]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>KRON4 by: Aubree Gordon, University of Michig...</td>\n",
       "      <td>[kron, aubree, gordon, university, michigan, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-31 23:11:29+00:00</td>\n",
       "      <td>{'id': 798925214, 'id_str': '798925214', 'name...</td>\n",
       "      <td>1223383328843280384</td>\n",
       "      <td>{'created_at': 'Fri Jan 31 20:41:20 +0000 2020...</td>\n",
       "      <td>#coronavirus #vaccine:</td>\n",
       "      <td>https://twitter.com/user/status/12233833288432...</td>\n",
       "      <td>{'hashtags': [{'text': 'Coronavirus', 'indices...</td>\n",
       "      <td>(6, 0.9883827140131066)</td>\n",
       "      <td>[https://www.cnbc.com/2020/01/31/coronavirus-w...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>798925214</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>[Coronavirus, vaccine]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>U.S. and international health officials are s...</td>\n",
       "      <td>[international, health, officials, speeding, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-31 23:12:03+00:00</td>\n",
       "      <td>{'id': 806144538049970176, 'id_str': '80614453...</td>\n",
       "      <td>1223383471999127552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233834719991...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(74, 0.9999928079220538)</td>\n",
       "      <td>[https://www.businessinsider.com/australia-suc...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>806144538049970176</td>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>A leading-edge research firm focused on digit...</td>\n",
       "      <td>[leading_edge, research, firm_focused, active,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-31 23:35:42+00:00</td>\n",
       "      <td>{'id': 61298849, 'id_str': '61298849', 'name':...</td>\n",
       "      <td>1223389423045206016</td>\n",
       "      <td>{'created_at': 'Tue Jan 28 18:26:16 +0000 2020...</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233894230452...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(36, 0.9999993854913211)</td>\n",
       "      <td>[https://www.greenmedinfo.com/blog/examining-r...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>61298849</td>\n",
       "      <td>36</td>\n",
       "      <td>True</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>Mr. Kennedy is in very safe territory by rep...</td>\n",
       "      <td>[mr, kennedy, safe, territory, reporting, cdc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-31 23:37:23+00:00</td>\n",
       "      <td>{'id': 1152822375567654912, 'id_str': '1152822...</td>\n",
       "      <td>1223389844719599616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>https://twitter.com/user/status/12233898447195...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>(74, 0.9999985797901156)</td>\n",
       "      <td>[https://www.dailymail.co.uk/news/article-7952...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1152822375567654912</td>\n",
       "      <td>74</td>\n",
       "      <td>False</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>By Kylie Stevens and Stephen Gibbs and Nic Wh...</td>\n",
       "      <td>[kylie, stevens, stephen, gibbs, nic, white, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  \\\n",
       "0  2020-01-31 23:08:33+00:00   \n",
       "1  2020-01-31 23:11:29+00:00   \n",
       "2  2020-01-31 23:12:03+00:00   \n",
       "3  2020-01-31 23:35:42+00:00   \n",
       "4  2020-01-31 23:37:23+00:00   \n",
       "\n",
       "                                                user               id_str  \\\n",
       "0  {'id': 19031057, 'id_str': '19031057', 'name':...  1223382589689356288   \n",
       "1  {'id': 798925214, 'id_str': '798925214', 'name...  1223383328843280384   \n",
       "2  {'id': 806144538049970176, 'id_str': '80614453...  1223383471999127552   \n",
       "3  {'id': 61298849, 'id_str': '61298849', 'name':...  1223389423045206016   \n",
       "4  {'id': 1152822375567654912, 'id_str': '1152822...  1223389844719599616   \n",
       "\n",
       "                                    retweeted_status              Hash words  \\\n",
       "0                                                NaN            #coronavirus   \n",
       "1  {'created_at': 'Fri Jan 31 20:41:20 +0000 2020...  #coronavirus #vaccine:   \n",
       "2                                                NaN             No hashtags   \n",
       "3  {'created_at': 'Tue Jan 28 18:26:16 +0000 2020...             No hashtags   \n",
       "4                                                NaN             No hashtags   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://twitter.com/user/status/12233825896893...   \n",
       "1  https://twitter.com/user/status/12233833288432...   \n",
       "2  https://twitter.com/user/status/12233834719991...   \n",
       "3  https://twitter.com/user/status/12233894230452...   \n",
       "4  https://twitter.com/user/status/12233898447195...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'hashtags': [{'text': 'coronavirus', 'indices...   \n",
       "1  {'hashtags': [{'text': 'Coronavirus', 'indices...   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                Topic Label  \\\n",
       "0   (43, 0.999852366207409)   \n",
       "1   (6, 0.9883827140131066)   \n",
       "2  (74, 0.9999928079220538)   \n",
       "3  (36, 0.9999993854913211)   \n",
       "4  (74, 0.9999985797901156)   \n",
       "\n",
       "                                                urls  no_link  ...  \\\n",
       "0  [https://www.kron4.com/news/national/when-will...    False  ...   \n",
       "1  [https://www.cnbc.com/2020/01/31/coronavirus-w...    False  ...   \n",
       "2  [https://www.businessinsider.com/australia-suc...    False  ...   \n",
       "3  [https://www.greenmedinfo.com/blog/examining-r...    False  ...   \n",
       "4  [https://www.dailymail.co.uk/news/article-7952...    False  ...   \n",
       "\n",
       "   day_of_tweet month_year week_month_year              user_id cluster  \\\n",
       "0    2020-01-31    2020-01               5             19031057      43   \n",
       "1    2020-01-31    2020-01               5            798925214       6   \n",
       "2    2020-01-31    2020-01               5   806144538049970176      74   \n",
       "3    2020-01-31    2020-01               5             61298849      36   \n",
       "4    2020-01-31    2020-01               5  1152822375567654912      74   \n",
       "\n",
       "   is_retweet                hashtags  link_shorteners  \\\n",
       "0       False           [coronavirus]          [False]   \n",
       "1        True  [Coronavirus, vaccine]          [False]   \n",
       "2       False                  [None]          [False]   \n",
       "3        True                  [None]          [False]   \n",
       "4       False                  [None]           [True]   \n",
       "\n",
       "                                        article_text  \\\n",
       "0   KRON4 by: Aubree Gordon, University of Michig...   \n",
       "1   U.S. and international health officials are s...   \n",
       "2   A leading-edge research firm focused on digit...   \n",
       "3    Mr. Kennedy is in very safe territory by rep...   \n",
       "4   By Kylie Stevens and Stephen Gibbs and Nic Wh...   \n",
       "\n",
       "                                article_text_cleaned  \n",
       "0  [kron, aubree, gordon, university, michigan, c...  \n",
       "1  [international, health, officials, speeding, w...  \n",
       "2  [leading_edge, research, firm_focused, active,...  \n",
       "3  [mr, kennedy, safe, territory, reporting, cdc,...  \n",
       "4  [kylie, stevens, stephen, gibbs, nic, white, d...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(texts):\n",
    "    \"\"\"Takes a list of lists which contain strings (lemmatized/tokenized)\"\"\"\n",
    "    \"\"\"This dictionary is often referred to as id2word in NLP posts\"\"\"\n",
    "    # uses gensim.corpora.Dictionary\n",
    "    return corpora.Dictionary(texts)\n",
    "\n",
    "def make_bow_corpus(texts, dictionary):\n",
    "    \"\"\"Creates bag of words model\"\"\"\n",
    "    \"\"\"texts is a list of lists; doc is a list of strings\"\"\"\n",
    "    \"\"\"dictionary made using make_dict\"\"\"\n",
    "    # uses gensim\n",
    "    return [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = make_dict(df['article_text_cleaned'].tolist())\n",
    "articles_corpus = make_bow_corpus(df['article_text_cleaned'].tolist(), articles_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save df\n",
    "DATA_PATH = './data/cleaned/'\n",
    "DF_NAME = 'concatenated_df_cleaned.pkl'\n",
    "\n",
    "with open(DATA_PATH + DF_NAME, 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save dictionary and bow_corpus\n",
    "articles_dict_filename = 'articles_dict.pkl'\n",
    "with open(DATA_PATH + articles_dict_filename, 'wb') as f:\n",
    "    pickle.dump(articles_dict, f)\n",
    "\n",
    "articles_corpus_filename = 'articles_corpus.pkl'\n",
    "with open(DATA_PATH + articles_corpus_filename, 'wb') as f:\n",
    "    pickle.dump(articles_corpus, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copied from 'make_doc2vec_model.py' in monster directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2vec model of monster jobs to calculate similarity\n",
    "\n",
    "### imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "# from gensim.utils import simple_preprocess\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# STOPWORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "class TaggedDocumentIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc, tags=[self.labels_list[idx]])\n",
    "\n",
    "\n",
    "\n",
    "### load data\n",
    "CLEAN_DATA_PATH = '/Users/richardkuzma/coding/analysis/monster/data/cleaned/'\n",
    "# load cleaned text\n",
    "with open(CLEAN_DATA_PATH + 'monster_jobs_cleaned_text.pkl', 'rb') as f:\n",
    "    cleaned_text = pickle.load(f)\n",
    "\n",
    "# load df\n",
    "with open(CLEAN_DATA_PATH + 'monster_jobs_df_small.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df['cleaned_description'] = cleaned_text\n",
    "docs = list(df['cleaned_description'])\n",
    "labels = list(df['id'])\n",
    "\n",
    "sentences = TaggedDocumentIterator(docs, labels)\n",
    "\n",
    "\n",
    "# create doc2vec model\n",
    "model = Doc2Vec(vector_size=100,\n",
    "                 window=5,\n",
    "                 min_count=20,\n",
    "                 workers=1,\n",
    "                 alpha=0.025,\n",
    "#                  min_alpha=0.0025,\n",
    "                 epochs=100,\n",
    "                 dm=0,        #1 = paragraph vector - distributed memory; 0 = dbow\n",
    "                 seed=42)\n",
    "\n",
    "# train model\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences,\n",
    "             total_examples=model.corpus_count,\n",
    "             epochs=model.epochs)\n",
    "\n",
    "print('created Doc2Vec model')\n",
    "print('Distributed Mem (True) or BOW (false): {}'.format(model.dm))\n",
    "print('vector length: {}'.format(model.vector_size))\n",
    "print('corpus count: {}'.format(model.corpus_count))\n",
    "print('epochs: {}'.format(model.epochs))\n",
    "\n",
    "\n",
    "print('saving d2v model...')\n",
    "MODEL_PATH = '/Users/richardkuzma/coding/analysis/monster/models/'\n",
    "model_name = 'd2v_' + 'dm_' + str(int(model.dm)) +'_vecsize_' + str(model.vector_size) + '_epochs_' + str(model.epochs) + '.model'\n",
    "model.save(MODEL_PATH + model_name)\n",
    "print('saved d2v model. \\n location: ' + MODEL_PATH + '\\nname: ' + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_anti_vax",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
