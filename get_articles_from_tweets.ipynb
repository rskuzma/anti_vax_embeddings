{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline\n",
    "\n",
    "#### Get text data\n",
    "- ingest csv to df\n",
    "- drop tweets without URLs\n",
    "- ignore youtube and twitter urls\n",
    "- for a given tweet with a url:\n",
    "    - for a given url\n",
    "    - get year, month, date from 2020-01-31 23:04:55+00:00 format\n",
    "    - request url content\n",
    "    - scrape and aggregate paragraph tags or article tag\n",
    "    - preprocess text\n",
    "    - save text in df\n",
    "- load google news pretrained d2v model\n",
    "    - embed text from each article\n",
    "    - store in dataframe with url, embedding, text?, year, month, week\n",
    "- url_df\n",
    "    - t-sne or PCA by month\n",
    "    - https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "import ast\n",
    "import string, re, requests, urllib3\n",
    "try:\n",
    "    from unshortenit import UnshortenIt\n",
    "    unshortener_available = True\n",
    "except:\n",
    "    unshortener_available = False\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as reqs\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join('./data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = 10000\n",
    "SKIP = 100000\n",
    "\n",
    "# CSV_name = 'CovidVaxTweetsWithGSDMMTopicsMayJune'\n",
    "CSV_name = 'CovidVaxTweetsWithGSDMMTopics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep column names\n",
    "df_with_cols = pd.read_csv('./data/' + CSV_name + '.csv', nrows=50)\n",
    "df_colnames = list(df_with_cols.columns)\n",
    "\n",
    "# load big df\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_big = pd.read_csv('./data/' + CSV_name + '.csv', skiprows = SKIP, nrows=NROWS, names = df_colnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV_name = 'CovidVaxTweetsWithGSDMMTopicsMayJune'\n",
    "\n",
    "# skip 500k --> jan 27\n",
    "# skip 400k --> feb 13\n",
    "# skip 300k --> feb 29\n",
    "# skip 2\n",
    "# skip 275k --> apr 04\n",
    "# skip 250k --> apr 10\n",
    "# skip 200k --> apr25\n",
    "# skip 170k --> may01\n",
    "# skip 150k --> may05\n",
    "# skip 100k --> may 15\n",
    "# skip 50k  --> may 21\n",
    "# skip 0k   --> may 31\n",
    "\n",
    "# CSV_name = 'CovidVaxTweetsWithGSDMMTopics'\n",
    "# skip 100k --> 29FEB\n",
    "\n",
    "\n",
    "\n",
    "### currently have...\n",
    "# january 31  -- 1182 articles\n",
    "# february 13 -- 1367 articles\n",
    "# april 01    -- 532 articles\n",
    "# april 10    -- 567 articles\n",
    "# may 31      -- 897 articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downselect to columns we want, so speed up calculations\n",
    "cols_to_keep = ['created_at', 'user', 'id_str', 'retweeted_status', 'Hash words', 'link', 'entities', 'Topic Label']\n",
    "df = df_big[cols_to_keep]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    if urlparse(url).netloc.lower() in link_shorteners:\n",
    "        if unshortener_available:\n",
    "            try:\n",
    "                unshortener = UnshortenIt(default_timeout=3)\n",
    "                base_url = unshortener.unshorten(url, unshorten_nested=True)\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "        else:\n",
    "            urllib3.disable_warnings()\n",
    "            session = requests.Session()\n",
    "            try:\n",
    "                resp = session.head(url, allow_redirects=True, timeout=3.0, verify=False)\n",
    "                base_url = resp.url\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "def process_url(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            new_urls =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    new_url = get_base_url(u)\n",
    "                    new_urls.append(new_url)\n",
    "            else:\n",
    "                new_urls = ['None']\n",
    "        else:\n",
    "            new_urls = ['None']\n",
    "    else:\n",
    "        new_urls = ['None']\n",
    "        \n",
    "    return new_urls\n",
    "\n",
    "\n",
    "def get_youtube_id(urls):\n",
    "    you_tube_ids =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            query = urlparse(url)\n",
    "            if query.hostname == 'youtu.be':\n",
    "                you_tube_ids.append(query.path[1:])\n",
    "            elif query.hostname in ('www.youtube.com', 'youtube.com', 'm.youtube.com'):\n",
    "                p = parse_qs(query.query)\n",
    "                you_tube_ids.append(p.get('v',['None'])[0])\n",
    "            else:\n",
    "                you_tube_ids.append('None')\n",
    "        else:\n",
    "            you_tube_ids.append('None')\n",
    "    return you_tube_ids\n",
    "\n",
    "\n",
    "def get_domain(urls):\n",
    "    domains =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            domains.append(urlparse(url).netloc.lower())\n",
    "        else:\n",
    "            domains.append('None')\n",
    "            \n",
    "    return domains\n",
    "        \n",
    "\n",
    "link_shorteners =['trib.al', 'bit.ly','www.bit.ly','tinyurl','ow.ly','buff.ly',\n",
    "                           'rebrand.ly', 'dlvr.it','sco.lt', 'shar.es', 'spr.ly',\n",
    "                           'zpr.io', 'zurl.co', 'tinyurl.com', 'ht.ly', 'youtu.be',\n",
    "                           't.ly', 'snip.ly', 'qoo.ly', 'loom.ly', 'invst.ly',\n",
    "                           'hubs.ly', 'gates.ly', 'frost.ly', 'fcld.ly', 'cutt.ly',\n",
    "                           'all.be', 'amzn.to', 'goo.gl', 'is.gd', 'bit.do', 'mcaf.ee',\n",
    "                           'shorte.st', 'bc.vc', 'lnkd.in', 't.co', 'ift.tt', 'flip.it',\n",
    "                           'reut.rs', 'nyti.ms', 'chng.it', 'cnn.it', 'cnb.cx', 'mol.im',\n",
    "                           'paper.li', 'toi.in', 'flip.it', 'hill.cm', 'bbc.in',\n",
    "                           'ti.me', 'politi.co', 'aje.io', 'gizmo.do', 'youtu.be']    \n",
    "\n",
    "def check_for_link_shortener(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            shortened =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    if urlparse(u).netloc.lower() in link_shorteners:\n",
    "                        shortened.append('True')\n",
    "                    else:\n",
    "                        shortened.append('False')\n",
    "            else:\n",
    "                shortened = ['None']\n",
    "        else:\n",
    "            shortened = ['None']\n",
    "    else:\n",
    "        shortened = ['None']\n",
    "        \n",
    "    return shortened\n",
    "\n",
    "def process_hashtags(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"hashtags\" in ents:\n",
    "            hashtags_info = ents[\"hashtags\"]\n",
    "            if len(hashtags_info) > 0:\n",
    "                hashtags=[]\n",
    "                for h in hashtags_info:\n",
    "                    hashtags.append(h[\"text\"])\n",
    "            else:\n",
    "                hashtags = ['None']\n",
    "        else:\n",
    "            hashtags = ['None']\n",
    "    else:\n",
    "        hashtags = ['None']\n",
    "        \n",
    "    return hashtags\n",
    "\n",
    "def no_link(urls: list):\n",
    "    if urls == ['None']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def twitter_or_youtube_in_domain(domain_list: list):\n",
    "    for domain in domain_list: # deal with one, not entire list\n",
    "        if ('twitter.com' in domain) or ('youtube.com' in domain):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pull_first_url(url_list: list):\n",
    "    return url_list[0]\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "# signal.signal(signal.SIGALRM, signal_handler)\n",
    "# signal.alarm(300)   # Ten seconds\n",
    "# try:\n",
    "#     long_function_call()\n",
    "# except Exception, msg:\n",
    "#     print \"Timed out!\"\n",
    "\n",
    "def get_article_text_from_url_list(link_list: list):\n",
    "    count = 1\n",
    "    link = link_list[0] # I made it so we're only looking at the first URL in list\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(11)   # 11 sec\n",
    "    try:\n",
    "        r = requests.get(link, timeout=6)\n",
    "        content = r.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        temp = ''\n",
    "        for tag in soup.find_all('p'):\n",
    "            temp = temp + ' ' + tag.get_text()\n",
    "        r.close() \n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return temp\n",
    "    except:\n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return \"exception occurred\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_df):\n",
    "    print(data_df.shape)\n",
    "    # process urls first\n",
    "    data_df['urls'] = data_df['entities'].apply(process_url)\n",
    "\n",
    "    \n",
    "    # identify which tweets have no link. remove them\n",
    "    data_df['no_link'] = data_df['urls'].apply(lambda x: no_link(x))\n",
    "    data_df = data_df[data_df['no_link'] == False]\n",
    "    print('links only: {}'.format(data_df.shape))\n",
    "    \n",
    "    # identify twitter or youtube links (can't be embed). Remove them\n",
    "    data_df['twitter_or_youtube'] = data_df['urls'].apply(lambda x: twitter_or_youtube_in_domain(x))\n",
    "    data_df = data_df[data_df['twitter_or_youtube'] == False]\n",
    "    print('no youtube or twitter links: {}'.format(data_df.shape))\n",
    "    \n",
    "    # pull only first url from list for simplicity\n",
    "    data_df['first_url'] = data_df['urls'].apply(lambda x: pull_first_url(x))\n",
    "    \n",
    "    # iain's processing\n",
    "    data_df['domains']=data_df['urls'].apply(get_domain)\n",
    "    data_df['day_of_tweet'] = pd.to_datetime(data_df['created_at']).dt.to_period('D')\n",
    "    data_df['month_year'] = pd.to_datetime(data_df['created_at']).dt.to_period('M')\n",
    "    data_df['week_month_year'] = pd.to_datetime(data_df['created_at']).dt.weekofyear\n",
    "    data_df['user'] = data_df['user'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['Topic Label'] = data_df['Topic Label'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['user_id'] = data_df['user'].map(lambda x: x['id_str'])\n",
    "    data_df['cluster'] = data_df['Topic Label'].map(lambda x: x[0])\n",
    "    data_df['is_retweet'] = ~data_df['retweeted_status'].isnull()\n",
    "    data_df['hashtags']=data_df['entities'].apply(process_hashtags)\n",
    "    data_df['link_shorteners'] = data_df['entities'].apply(check_for_link_shortener)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def scrape_articles(data_df):\n",
    "    # extract text using requests and beautiful soup... leaves something to be desired\n",
    "    data_df['article_text'] = data_df['urls'].progress_apply(lambda x: get_article_text_from_url_list(x))\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def drop_short_articles(data_df):    \n",
    "    index_names = data_df[data_df['article_text'].str.len() < 500].index\n",
    "    data_df.drop(index_names, inplace = True)\n",
    "    \n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data(df)\n",
    "df_len = len(df)\n",
    "with open('./data/cleaned/' + CSV_name + '_' + str(SKIP) + '_skip_' + str(NROWS) + '_rows_' + str(df_len) + '_processed.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = scrape_articles(df)\n",
    "# df_articles = len(df)\n",
    "# with open('./data/cleaned/' + CSV_name + '_' + str(NROWS) + '_rows_' + str(df_len) + '_processed_'+ str(df_articles) + '_articles.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = drop_short_articles(df)\n",
    "# df_articles_left = len(df)\n",
    "# with open('./data/cleaned/' + CSV_name + '_' + str(NROWS) + '_rows_' + str(df_len) + '_processed_'+ str(df_articles) + '_articles_' + str(df_articles_left) + '_left.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart Kernal, then start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "import ast\n",
    "import string, re, requests, urllib3\n",
    "try:\n",
    "    from unshortenit import UnshortenIt\n",
    "    unshortener_available = True\n",
    "except:\n",
    "    unshortener_available = False\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as reqs\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import signal\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join('./data/'))\n",
    "\n",
    "def get_base_url(url):\n",
    "    if urlparse(url).netloc.lower() in link_shorteners:\n",
    "        if unshortener_available:\n",
    "            try:\n",
    "                unshortener = UnshortenIt(default_timeout=3)\n",
    "                base_url = unshortener.unshorten(url, unshorten_nested=True)\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "        else:\n",
    "            urllib3.disable_warnings()\n",
    "            session = requests.Session()\n",
    "            try:\n",
    "                resp = session.head(url, allow_redirects=True, timeout=3.0, verify=False)\n",
    "                base_url = resp.url\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "def process_url(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            new_urls =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    new_url = get_base_url(u)\n",
    "                    new_urls.append(new_url)\n",
    "            else:\n",
    "                new_urls = ['None']\n",
    "        else:\n",
    "            new_urls = ['None']\n",
    "    else:\n",
    "        new_urls = ['None']\n",
    "        \n",
    "    return new_urls\n",
    "\n",
    "\n",
    "def get_youtube_id(urls):\n",
    "    you_tube_ids =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            query = urlparse(url)\n",
    "            if query.hostname == 'youtu.be':\n",
    "                you_tube_ids.append(query.path[1:])\n",
    "            elif query.hostname in ('www.youtube.com', 'youtube.com', 'm.youtube.com'):\n",
    "                p = parse_qs(query.query)\n",
    "                you_tube_ids.append(p.get('v',['None'])[0])\n",
    "            else:\n",
    "                you_tube_ids.append('None')\n",
    "        else:\n",
    "            you_tube_ids.append('None')\n",
    "    return you_tube_ids\n",
    "\n",
    "\n",
    "def get_domain(urls):\n",
    "    domains =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            domains.append(urlparse(url).netloc.lower())\n",
    "        else:\n",
    "            domains.append('None')\n",
    "            \n",
    "    return domains\n",
    "        \n",
    "\n",
    "link_shorteners =['trib.al', 'bit.ly','www.bit.ly','tinyurl','ow.ly','buff.ly',\n",
    "                           'rebrand.ly', 'dlvr.it','sco.lt', 'shar.es', 'spr.ly',\n",
    "                           'zpr.io', 'zurl.co', 'tinyurl.com', 'ht.ly', 'youtu.be',\n",
    "                           't.ly', 'snip.ly', 'qoo.ly', 'loom.ly', 'invst.ly',\n",
    "                           'hubs.ly', 'gates.ly', 'frost.ly', 'fcld.ly', 'cutt.ly',\n",
    "                           'all.be', 'amzn.to', 'goo.gl', 'is.gd', 'bit.do', 'mcaf.ee',\n",
    "                           'shorte.st', 'bc.vc', 'lnkd.in', 't.co', 'ift.tt', 'flip.it',\n",
    "                           'reut.rs', 'nyti.ms', 'chng.it', 'cnn.it', 'cnb.cx', 'mol.im',\n",
    "                           'paper.li', 'toi.in', 'flip.it', 'hill.cm', 'bbc.in',\n",
    "                           'ti.me', 'politi.co', 'aje.io', 'gizmo.do', 'youtu.be']    \n",
    "\n",
    "def check_for_link_shortener(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            shortened =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    if urlparse(u).netloc.lower() in link_shorteners:\n",
    "                        shortened.append('True')\n",
    "                    else:\n",
    "                        shortened.append('False')\n",
    "            else:\n",
    "                shortened = ['None']\n",
    "        else:\n",
    "            shortened = ['None']\n",
    "    else:\n",
    "        shortened = ['None']\n",
    "        \n",
    "    return shortened\n",
    "\n",
    "def process_hashtags(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"hashtags\" in ents:\n",
    "            hashtags_info = ents[\"hashtags\"]\n",
    "            if len(hashtags_info) > 0:\n",
    "                hashtags=[]\n",
    "                for h in hashtags_info:\n",
    "                    hashtags.append(h[\"text\"])\n",
    "            else:\n",
    "                hashtags = ['None']\n",
    "        else:\n",
    "            hashtags = ['None']\n",
    "    else:\n",
    "        hashtags = ['None']\n",
    "        \n",
    "    return hashtags\n",
    "\n",
    "def no_link(urls: list):\n",
    "    if urls == ['None']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def twitter_or_youtube_in_domain(domain_list: list):\n",
    "    for domain in domain_list: # deal with one, not entire list\n",
    "        if ('twitter.com' in domain) or ('youtube.com' in domain):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pull_first_url(url_list: list):\n",
    "    return url_list[0]\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "# signal.signal(signal.SIGALRM, signal_handler)\n",
    "# signal.alarm(300)   # Ten seconds\n",
    "# try:\n",
    "#     long_function_call()\n",
    "# except Exception, msg:\n",
    "#     print \"Timed out!\"\n",
    "\n",
    "def get_article_text_from_url_list(link_list: list):\n",
    "    count = 1\n",
    "    link = link_list[0] # I made it so we're only looking at the first URL in list\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(11)   # 11 sec\n",
    "    try:\n",
    "        r = requests.get(link, timeout=6)\n",
    "        content = r.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        temp = ''\n",
    "        for tag in soup.find_all('p'):\n",
    "            temp = temp + ' ' + tag.get_text()\n",
    "        r.close() \n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return temp\n",
    "    except:\n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return \"exception occurred\"\n",
    "\n",
    "\n",
    "\n",
    "def process_data(data_df):\n",
    "    print(data_df.shape)\n",
    "    # process urls first\n",
    "    data_df['urls'] = data_df['entities'].apply(process_url)\n",
    "\n",
    "    \n",
    "    # identify which tweets have no link. remove them\n",
    "    data_df['no_link'] = data_df['urls'].apply(lambda x: no_link(x))\n",
    "    data_df = data_df[data_df['no_link'] == False]\n",
    "    print('links only: {}'.format(data_df.shape))\n",
    "    \n",
    "    # identify twitter or youtube links (can't be embed). Remove them\n",
    "    data_df['twitter_or_youtube'] = data_df['urls'].apply(lambda x: twitter_or_youtube_in_domain(x))\n",
    "    data_df = data_df[data_df['twitter_or_youtube'] == False]\n",
    "    print('no youtube or twitter links: {}'.format(data_df.shape))\n",
    "    \n",
    "    # pull only first url from list for simplicity\n",
    "    data_df['first_url'] = data_df['urls'].apply(lambda x: pull_first_url(x))\n",
    "    \n",
    "    # iain's processing\n",
    "    data_df['domains']=data_df['urls'].apply(get_domain)\n",
    "    data_df['day_of_tweet'] = pd.to_datetime(data_df['created_at']).dt.to_period('D')\n",
    "    data_df['month_year'] = pd.to_datetime(data_df['created_at']).dt.to_period('M')\n",
    "    data_df['week_month_year'] = pd.to_datetime(data_df['created_at']).dt.weekofyear\n",
    "    data_df['user'] = data_df['user'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['Topic Label'] = data_df['Topic Label'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['user_id'] = data_df['user'].map(lambda x: x['id_str'])\n",
    "    data_df['cluster'] = data_df['Topic Label'].map(lambda x: x[0])\n",
    "    data_df['is_retweet'] = ~data_df['retweeted_status'].isnull()\n",
    "    data_df['hashtags']=data_df['entities'].apply(process_hashtags)\n",
    "    data_df['link_shorteners'] = data_df['entities'].apply(check_for_link_shortener)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def scrape_articles(data_df):\n",
    "    # extract text using requests and beautiful soup... leaves something to be desired\n",
    "    data_df['article_text'] = data_df['urls'].progress_apply(lambda x: get_article_text_from_url_list(x))\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def drop_short_articles(data_df):    \n",
    "    index_names = data_df[data_df['article_text'].str.len() < 500].index\n",
    "    data_df.drop(index_names, inplace = True)\n",
    "    \n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len = 1548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NROWS = 10000\n",
    "SKIP = 100000\n",
    "\n",
    "# CSV_name = 'CovidVaxTweetsWithGSDMMTopicsMayJune'\n",
    "CSV_name = 'CovidVaxTweetsWithGSDMMTopics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filepath = './data/cleaned/' + CSV_name + '_' + str(SKIP) +'_skip_' + str(NROWS) + '_rows_' + str(df_len) + '_processed.pkl'\n",
    "with open(df_filepath, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df_len = len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = scrape_articles(df)\n",
    "df_articles = len(df)\n",
    "with open('./data/cleaned/' + CSV_name + '_' + str(SKIP) + '_skip_' + str(NROWS) + '_rows_' + str(df_len) + '_processed_'+ str(df_articles) + '_articles.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_short_articles(df)\n",
    "df_articles_left = len(df)\n",
    "with open('./data/cleaned/' + CSV_name + '_' + str(SKIP) + '_skip_' + str(NROWS) + '_rows_' + str(df_len) + '_processed_'+ str(df_articles) + '_articles_' + str(df_articles_left) + '_left.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15MAY2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking Lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### useful commands\n",
    "\n",
    "\n",
    "# r = requests.get(str_url)\n",
    "# content = r.content\n",
    "# soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# print(soup.prettify())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains(url):\n",
    "    domains = []\n",
    "    print(url)\n",
    "    print(urlparse(url).hostname)\n",
    "    domains.append(urlparse(url).hostname)\n",
    "        \n",
    "    return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls['domains'] = urls['url'].apply(get_domains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(urls['url'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://www.businessinsider.com/australia-successfully-grows-wuhan-coronavirus-sample-from-sick-patient-2020-1',\n",
    "         'https://www.dailymail.co.uk/news/article-7952287/Virologist-warns-coronavirus-deadlier-Qantas-waits-fly-Australians-Wuhan.html',\n",
    "         'https://news.abs-cbn.com/overseas/02/01/20/coronavirus-vaccine-will-take-months-biotech-exec',\n",
    "         'https://www.politico.com/news/2020/01/31/coronavirus-vaccine-missed-chance-109709']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = []\n",
    "\n",
    "\n",
    "for i in url_list:\n",
    "    soup = BeautifulSoup(requests.get(i).content, 'html.parser')\n",
    "    soups.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_article(soup):    \n",
    "    articles = soup.find_all('article')\n",
    "    for i in articles:\n",
    "        print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_paragraphs(soup):    \n",
    "    paragraphs = soup.find_all('p')\n",
    "    for i in paragraphs:\n",
    "        print(i.get_text())\n",
    "        print('-'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(soups)):\n",
    "    print('='*50)\n",
    "    print(url_list[i])\n",
    "    print('='*50)\n",
    "    show_article(soups[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(soups)):\n",
    "    print('='*50)\n",
    "    print(url_list[i])\n",
    "    print('='*50)\n",
    "    show_paragraphs(soups[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_content(soup):\n",
    "    articles = soup.find_all('article')\n",
    "    if len(articles) > 0:\n",
    "        for i in articles:\n",
    "            print(i.get_text())\n",
    "    else:\n",
    "        print('using paragraphs instead')\n",
    "        show_paragraphs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(soups)):\n",
    "    print('='*50)\n",
    "    print(url_list[i])\n",
    "    print('='*50)\n",
    "    show_content(soups[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups[0].find('article').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ''\n",
    "for i in range(len(soups[0].find_all('p'))):\n",
    "    temp = temp + soups[0].find_all('p')[i].get_text()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(soups)):\n",
    "    temp = ''\n",
    "    for j in range(len(soups[i].find_all('p'))):\n",
    "        temp = temp + soups[i].find_all('p')[j].get_text()\n",
    "    print(temp)\n",
    "    print('\\n'*3 + '='*60 + '\\n'*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text_from_url(link: str):\n",
    "    r = requests.get(link)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    temp = ''\n",
    "    for tag in soup.find_all('p'):\n",
    "        temp = temp + tag.get_text()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_article_text_from_url('https://www.dailymail.co.uk/news/article-7952287/Virologist-warns-coronavirus-deadlier-Qantas-waits-fly-Australians-Wuhan.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove pretrained\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = ‘glove.6B.100d.txt’\n",
    "word2vec_output_file = ‘glove.6B.100d.txt.word2vec’\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "You now have a copy of the GloVe model in word2vec format with the filename glove.6B.100d.txt.word2vec.\n",
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = ‘glove.6B.100d.txt.word2vec’\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "\n",
    "\n",
    "# load keyedvectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=10 ** 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_anti_vax",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
