{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Tweets for Anti-Vax Article Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/seagate_external_drive/anti_vax_embeddings/env_anti_vaccine/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/richardkuzma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "import ast\n",
    "import string, re, requests, urllib3\n",
    "try:\n",
    "    from unshortenit import UnshortenIt\n",
    "    unshortener_available = True\n",
    "except:\n",
    "    unshortener_available = False\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as reqs\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import signal\n",
    "\n",
    "### text_cleaning\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') # for part of speech tagging, required for lemmatization\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.models.phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure directory where data is stored is in the path\n",
    "sys.path.append(os.path.join('./data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out once ready for CMU machine\n",
    "NROWS = 10000 \n",
    "SKIP = 0\n",
    "\n",
    "CSV_name = 'CovidVaxTweetsWithGSDMMTopicsMayJune'\n",
    "# CSV_name = 'CovidVaxTweetsWithGSDMMTopics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "pd.set_option('display.max_columns', None)\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + CSV_name + '.csv', skiprows=SKIP, nrows=NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downselect to columns we want, so speed up calculations\n",
    "cols_to_keep = ['created_at', \n",
    "                'id_str', \n",
    "                'entities', \n",
    "                'user', \n",
    "                'retweet_count', \n",
    "                'favorite_count', \n",
    "#                 'retweeted', \n",
    "#                 'favorited', \n",
    "                'retweeted_status', \n",
    "                'Hash words', \n",
    "                'link', \n",
    "                'Topic Label']\n",
    "\n",
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_url(url):\n",
    "    if urlparse(url).netloc.lower() in link_shorteners:\n",
    "        if unshortener_available:\n",
    "            try:\n",
    "                unshortener = UnshortenIt(default_timeout=3)\n",
    "                base_url = unshortener.unshorten(url, unshorten_nested=True)\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "        else:\n",
    "            urllib3.disable_warnings()\n",
    "            session = requests.Session()\n",
    "            try:\n",
    "                resp = session.head(url, allow_redirects=True, timeout=3.0, verify=False)\n",
    "                base_url = resp.url\n",
    "                return base_url\n",
    "            except:\n",
    "                return 'unreachable_shortened_url'\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "def process_url(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            new_urls =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    new_url = get_base_url(u)\n",
    "                    new_urls.append(new_url)\n",
    "            else:\n",
    "                new_urls = ['None']\n",
    "        else:\n",
    "            new_urls = ['None']\n",
    "    else:\n",
    "        new_urls = ['None']\n",
    "        \n",
    "    return new_urls\n",
    "\n",
    "\n",
    "def get_youtube_id(urls):\n",
    "    you_tube_ids =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            query = urlparse(url)\n",
    "            if query.hostname == 'youtu.be':\n",
    "                you_tube_ids.append(query.path[1:])\n",
    "            elif query.hostname in ('www.youtube.com', 'youtube.com', 'm.youtube.com'):\n",
    "                p = parse_qs(query.query)\n",
    "                you_tube_ids.append(p.get('v',['None'])[0])\n",
    "            else:\n",
    "                you_tube_ids.append('None')\n",
    "        else:\n",
    "            you_tube_ids.append('None')\n",
    "    return you_tube_ids\n",
    "\n",
    "\n",
    "\n",
    "def get_domain(urls):\n",
    "    domains =[]\n",
    "    for url in urls:\n",
    "        if url !='None':\n",
    "            domains.append(urlparse(url).netloc.lower())\n",
    "        else:\n",
    "            domains.append('None')\n",
    "            \n",
    "    return domains\n",
    "        \n",
    "\n",
    "link_shorteners =['trib.al', 'bit.ly','www.bit.ly','tinyurl','ow.ly','buff.ly',\n",
    "                           'rebrand.ly', 'dlvr.it','sco.lt', 'shar.es', 'spr.ly',\n",
    "                           'zpr.io', 'zurl.co', 'tinyurl.com', 'ht.ly', 'youtu.be',\n",
    "                           't.ly', 'snip.ly', 'qoo.ly', 'loom.ly', 'invst.ly',\n",
    "                           'hubs.ly', 'gates.ly', 'frost.ly', 'fcld.ly', 'cutt.ly',\n",
    "                           'all.be', 'amzn.to', 'goo.gl', 'is.gd', 'bit.do', 'mcaf.ee',\n",
    "                           'shorte.st', 'bc.vc', 'lnkd.in', 't.co', 'ift.tt', 'flip.it',\n",
    "                           'reut.rs', 'nyti.ms', 'chng.it', 'cnn.it', 'cnb.cx', 'mol.im',\n",
    "                           'paper.li', 'toi.in', 'flip.it', 'hill.cm', 'bbc.in',\n",
    "                           'ti.me', 'politi.co', 'aje.io', 'gizmo.do', 'youtu.be']    \n",
    "\n",
    "def check_for_link_shortener(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"urls\" in ents:\n",
    "            urls = ents[\"urls\"]\n",
    "            shortened =[]\n",
    "            if len(urls) > 0:\n",
    "                for u in [url[\"expanded_url\"] for url in ents[\"urls\"]]:\n",
    "                    if urlparse(u).netloc.lower() in link_shorteners:\n",
    "                        shortened.append('True')\n",
    "                    else:\n",
    "                        shortened.append('False')\n",
    "            else:\n",
    "                shortened = ['None']\n",
    "        else:\n",
    "            shortened = ['None']\n",
    "    else:\n",
    "        shortened = ['None']\n",
    "        \n",
    "    return shortened\n",
    "\n",
    "def process_hashtags(entities):\n",
    "    ents = ast.literal_eval(entities)\n",
    "    if ents is not None:\n",
    "        if \"hashtags\" in ents:\n",
    "            hashtags_info = ents[\"hashtags\"]\n",
    "            if len(hashtags_info) > 0:\n",
    "                hashtags=[]\n",
    "                for h in hashtags_info:\n",
    "                    hashtags.append(h[\"text\"])\n",
    "            else:\n",
    "                hashtags = ['None']\n",
    "        else:\n",
    "            hashtags = ['None']\n",
    "    else:\n",
    "        hashtags = ['None']\n",
    "        \n",
    "    return hashtags\n",
    "\n",
    "def no_link(urls: list):\n",
    "    if urls == ['None']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def twitter_or_youtube_in_domain(domain_list: list):\n",
    "    for domain in domain_list: # deal with one, not entire list\n",
    "        if ('twitter.com' in domain) or ('youtube.com' in domain):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def pull_first_url(url_list: list):\n",
    "    return url_list[0]\n",
    "\n",
    "\n",
    "# def signal_handler(signum, frame):\n",
    "#     raise Exception(\"Timed out!\")\n",
    "\n",
    "# signal.signal(signal.SIGALRM, signal_handler)\n",
    "# signal.alarm(300)   # Ten seconds\n",
    "# try:\n",
    "#     long_function_call()\n",
    "# except Exception, msg:\n",
    "#     print \"Timed out!\"\n",
    "\n",
    "def get_article_text_from_url_list(link):\n",
    "    count = 1\n",
    "#     link = link_list[0] # I made it so we're only looking at the first URL in list\n",
    "    \n",
    "#     signal.signal(signal.SIGALRM, signal_handler)\n",
    "#     signal.alarm(11)   # 11 sec\n",
    "    try:\n",
    "        r = requests.get(link, timeout=6)\n",
    "        content = r.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        temp = ''\n",
    "        for tag in soup.find_all('p'):\n",
    "            temp = temp + ' ' + tag.get_text()\n",
    "        r.close() \n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return temp\n",
    "    except:\n",
    "#         print(count)\n",
    "#         count +=1\n",
    "        return \"exception occurred\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_df):\n",
    "    print('original shape: {}'.format(data_df.shape))\n",
    "    # process urls first\n",
    "    data_df['urls'] = data_df['entities'].apply(process_url)\n",
    "\n",
    "    \n",
    "    # identify which tweets have no link. remove them\n",
    "    data_df['no_link'] = data_df['urls'].apply(lambda x: no_link(x))\n",
    "    data_df = data_df[data_df['no_link'] == False]\n",
    "    print('links only: {}'.format(data_df.shape))\n",
    "    \n",
    "    # identify twitter or youtube links (can't be embed). Remove them\n",
    "    data_df['twitter_or_youtube'] = data_df['urls'].apply(lambda x: twitter_or_youtube_in_domain(x))\n",
    "    data_df = data_df[data_df['twitter_or_youtube'] == False]\n",
    "    print('no youtube or twitter links: {}'.format(data_df.shape))\n",
    "    \n",
    "    # pull only first url from list for simplicity\n",
    "    data_df['first_url'] = data_df['urls'].apply(lambda x: pull_first_url(x))\n",
    "    \n",
    "    # iain's processing\n",
    "    data_df['domains']=data_df['urls'].apply(get_domain)\n",
    "    data_df['day_of_tweet'] = pd.to_datetime(data_df['created_at']).dt.to_period('D')\n",
    "    data_df['month_year'] = pd.to_datetime(data_df['created_at']).dt.to_period('M')\n",
    "    data_df['week_month_year'] = pd.to_datetime(data_df['created_at']).dt.weekofyear\n",
    "    data_df['user'] = data_df['user'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['Topic Label'] = data_df['Topic Label'].apply(lambda x: ast.literal_eval(x))\n",
    "    data_df['user_id'] = data_df['user'].map(lambda x: x['id_str'])\n",
    "    data_df['cluster'] = data_df['Topic Label'].map(lambda x: x[0])\n",
    "    data_df['is_retweet'] = ~data_df['retweeted_status'].isnull()\n",
    "    data_df['hashtags']=data_df['entities'].apply(process_hashtags)\n",
    "    data_df['link_shorteners'] = data_df['entities'].apply(check_for_link_shortener)\n",
    "    \n",
    "    return data_df\n",
    "    \n",
    "def scrape_articles(data_df):\n",
    "    # extract text using requests and beautiful soup... leaves something to be desired\n",
    "    data_df['article_text'] = data_df['first_url'].progress_apply(lambda x: get_article_text_from_url_list(x))\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def drop_short_articles(data_df):    \n",
    "    index_names = data_df[data_df['article_text'].str.len() < 500].index\n",
    "    data_df.drop(index_names, inplace = True)\n",
    "    \n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: (10000, 12)\n",
      "links only: (1845, 14)\n",
      "no youtube or twitter links: (1339, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/seagate_external_drive/anti_vax_embeddings/env_anti_vaccine/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/Volumes/seagate_external_drive/anti_vax_embeddings/env_anti_vaccine/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py:1091: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  UserWarning,\n",
      "/Volumes/seagate_external_drive/anti_vax_embeddings/env_anti_vaccine/lib/python3.7/site-packages/ipykernel_launcher.py:24: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n"
     ]
    }
   ],
   "source": [
    "# Iain's processing\n",
    "df = process_data(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "### sum total retweets and total favorites for each url shared in a tweet\n",
    "df['url_total_retweets'] = df.groupby(['first_url'])['retweet_count'].transform('sum')\n",
    "df['url_total_favorites'] = df.groupby(['first_url'])['favorite_count'].transform('sum')\n",
    "\n",
    "\n",
    "### drop duplicate URLs\n",
    "df.drop_duplicates(subset=['first_url'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 145/964 [03:31<08:46,  1.56it/s] Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 17%|█▋        | 165/964 [03:50<05:55,  2.25it/s]  Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 49%|████▊     | 468/964 [14:23<14:12,  1.72s/it]   Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 74%|███████▍  | 711/964 [21:27<02:01,  2.08it/s]  Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 77%|███████▋  | 742/964 [22:13<06:33,  1.77s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      " 81%|████████  | 780/964 [23:28<03:07,  1.02s/it]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "100%|██████████| 964/964 [28:12<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "### scrape articles (6sec timeout)\n",
    "df = scrape_articles(df)    \n",
    "\n",
    "### drop short articles\n",
    "df = drop_short_articles(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_simple_preprocess(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [[word for word in simple_preprocess(str(doc), deacc = True) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_text_preprocessed'] = remove_stopwords_and_simple_preprocess(df['article_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to pkl\n",
    "df_name = 'tweets_urls_articles_cleaned.pkl'\n",
    "\n",
    "with open(DATA_PATH + df_name, 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_anti_vaccine",
   "language": "python",
   "name": "env_anti_vaccine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
