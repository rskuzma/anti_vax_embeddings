{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make average and weighted word embedding for each document\n",
    "- Load df\n",
    "- Load pretrained word vector model\n",
    "    - I used Glove, but FastText may be better\n",
    "- get centroids with glove vecs (300)\n",
    "- get glove x tfidf weighted vecs (300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load df\n",
    "\n",
    "DATA_PATH = './data/cleaned/'\n",
    "DF_NAME = 'concatenated_df_cleaned.pkl'\n",
    "\n",
    "with open(DATA_PATH + DF_NAME, 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load word embeddings model\n",
    "# Here I use the Stanford GloVe model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "MODEL_PATH = './models/'\n",
    "model_filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(MODEL_PATH + model_filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word_embedding(sent: str, model):\n",
    "    errors = 0\n",
    "    sent_sum = np.zeros(100)\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = model[word]\n",
    "            sent_sum += vec\n",
    "        except:\n",
    "            errors +=1\n",
    "            pass\n",
    "\n",
    "    sent_avg = sent_sum / len(sent_sum)\n",
    "    return sent_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['glove_avg'] = df['article_text_cleaned'].apply(lambda x: get_avg_word_embedding(x, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'concatenated_df_cleaned_glove.pkl'\n",
    "with open(DATA_PATH + filename, 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim tfidf giving problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# MODEL_PATH = './models/'\n",
    "# tfidf_name = 'tfidf.model'\n",
    "# tfidf = TfidfModel.load(MODEL_PATH + tfidf_name)\n",
    "\n",
    "# ### load dictionary\n",
    "# DATA_PATH = './data/cleaned/'\n",
    "# articles_dict_filename = 'articles_dict.pkl'\n",
    "\n",
    "# with open(DATA_PATH + articles_dict_filename, 'rb') as f:\n",
    "#     articles_dict = pickle.load(f)\n",
    "\n",
    "    \n",
    "# ### load corpus\n",
    "# articles_corpus_filename = 'articles_corpus.pkl'\n",
    "\n",
    "# with open(DATA_PATH + articles_corpus_filename, 'rb') as f:\n",
    "#     articles_corpus = pickle.load(f)\n",
    "\n",
    "# # fit to the corpus\n",
    "# corpus_tfidf = tfidf[articles_corpus]\n",
    "\n",
    "# print('type of tfidf model: {}'.format(type(tfidf)))\n",
    "# print('type of corpus_tfidf: {}'.format(type(corpus_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf from sklearn\n",
    "#### weighting word vectors with tf-idf takes too long... (12+ hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "df['article_text_cleaned_string'] = df['article_text_cleaned'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "# !pip install tdqm\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(stop_words=None)\n",
    "final_tf_idf = tf_idf_vect.fit_transform(df['article_text_cleaned_string'])\n",
    "tfidf_feat = tf_idf_vect.get_feature_names()\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "#Applying TF-IDF scores to the model vectors\n",
    "tfidf_sent_vectors = [] # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0\n",
    "errors=0\n",
    "\n",
    "\n",
    "for sent in tqdm(df['article_text_cleaned'].tolist()): # for each review/sentence\n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = model[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = final_tf_idf [row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))\n",
    "\n",
    "seconds = time.time()-t0\n",
    "minutes = seconds/60\n",
    "hours = minutes / 60\n",
    "hours_min = minutes % 60\n",
    "\n",
    "print('{} hours and {} minutes'.format(hours, hours_min))\n",
    "\n",
    "len(tfidf_sent_vectors)\n",
    "type(tfidf_sent_vectors)\n",
    "\n",
    "DATA_PATH = './data/cleaned/'\n",
    "filename = 'tfidf_sent_vectors'\n",
    "with open(DATA_PATH + filename, 'wb') as f:\n",
    "    pickle.dump(tfidf_sent_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parking lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_text_cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "word = 'vaccine'\n",
    "print(word)\n",
    "print('tfidf_feat.index(' + word + '): {}'.format(tfidf_feat.index(word)))\n",
    "tfidf_sklearn[[row, tfidf_feat.index(word)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "errors=0\n",
    "for sent in tqdm(tokens): # for each review/sentence\n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
    "    weight_sum = 0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = model[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tfidf = tfidf_sklearn [row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weight_sum += tfidf\n",
    "        except:\n",
    "            errors =+1\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    #print(np.isnan(np.sum(sent_vec)))\n",
    "tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1\n",
    "print('errors noted: '+str(errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_anti_vax",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
